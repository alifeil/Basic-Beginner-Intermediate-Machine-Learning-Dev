# -*- coding: utf-8 -*-
"""Alifia Feiling - Time Series.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CtRJZrabp5_CG00opIQXIF5_zeegeAPA

**DATA DIRI**

Nama : Alifia Feiling Asmoro Siwi

Username : alifiafas

IDCAMP : Machine Learning Developer
"""

from google.colab import drive
drive.mount('/content/drive')

import tensorflow as tf
tf.__version__

import numpy as np
import pandas as pd
from keras.layers import Dense, LSTM
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM,Dense,Bidirectional,Dropout
tf.keras.backend.set_floatx('float64')
from sklearn.model_selection import train_test_split
cuaca = pd.read_csv('/content/weather.csv')
cuaca

cuaca.values.shape

cuaca.isna().sum()

cuaca['meanku'] = cuaca[['temperatureHigh', 'temperatureLow']].mean(axis=1)
cuaca

tanggal = cuaca['time'].values
temperatur = cuaca['meanku'].values

x_train, x_valid, y_train, y_valid = train_test_split(temperatur, tanggal, train_size=0.8, test_size = 0.2, shuffle = False )

print('Total Data Train : ',len(x_train))
print('Total Data Validation : ',len(x_valid))

def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
  series = tf.expand_dims(series, axis=-1)
  ds = tf.data.Dataset.from_tensor_slices(series)
  ds = ds.window(window_size + 1, shift=1, drop_remainder = True)
  ds = ds.flat_map(lambda w: w.batch(window_size + 1))
  ds = ds.shuffle(shuffle_buffer)
  ds = ds.map(lambda w: (w[:-1], w[-1:]))
  return ds.batch(batch_size).prefetch(1)

train_set = windowed_dataset(x_train, window_size=64, batch_size=200, shuffle_buffer=1000)
val_set = windowed_dataset(x_valid, window_size=64, batch_size=200, shuffle_buffer=1000)

model = Sequential([
    Bidirectional(LSTM(60, return_sequences=True)),
    Bidirectional(LSTM(60)),
    Dense(30, activation="relu"),
    Dense(10, activation="relu"),
    Dense(1),
])

Mae = (cuaca['meanku'].max() - cuaca['meanku'].min()) * 10/100
print(Mae)

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('mae')<16.3 and logs.get('val_mae')<16.3):
      print("\nMAE dari model < 10% skala data")
      self.model.stop_training = True
callbacks = myCallback()

optimizer = tf.keras.optimizers.SGD(learning_rate=1.0000e-04, momentum=0.9)

model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=["mae"])

history = model.fit(train_set, epochs=6, validation_data = val_set, callbacks=[callbacks])

plt.plot(history.history['mae'])
plt.plot(history.history['val_mae'])
plt.title('Akurasi Model')
plt.ylabel('Mae')
plt.xlabel('epoch')
plt.legend(['Train', 'Val'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Loss Model')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['Train', 'Val'], loc='upper left')
plt.show()

"""### **Lampiran DATA DIRI di dicoding**"""

import matplotlib.image as mpimg
img = mpimg.imread('/content/Data diri Alifia Feiling.jpg')
imgplot = plt.imshow(img)
plt.show()