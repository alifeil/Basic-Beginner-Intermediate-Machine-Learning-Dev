# -*- coding: utf-8 -*-
"""Alifia Feiling Asmoro Siwi - NLP Dicoding.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UFrBMi-71BieTO7s2ieMPpisbpbpbi60

**DATA DIRI**

Nama : Alifia Feiling Asmoro Siwi

Username : alifiafas

IDCAMP : Machine Learning Developer
"""

from google.colab import drive
drive.mount('/content/drive')

import tensorflow as tf
tf.__version__

import numpy as np 
import pandas as pd 
import os
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.utils import shuffle
import string
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
from tensorflow.keras.layers import Activation, Dense, Embedding, GlobalAveragePooling1D
from keras.preprocessing.text import Tokenizer
from keras.models import Sequential
from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D
from sklearn.model_selection import train_test_split
from keras.utils.np_utils import to_categorical
from keras.callbacks import EarlyStopping
from keras.layers import Dropout
import re
from nltk.corpus import stopwords
from nltk import word_tokenize
import nltk
nltk.download('stopwords')
STOPWORDS = set(stopwords.words('english'))

consumer = pd.read_csv("/content/consumer.csv")
consumer

consumer.isnull().sum()

consumer.dtypes

consumer.shape

consumer.describe

plt.figure(figsize=(14,7))
sns.heatmap(consumer.corr(method='kendall'), annot=True )

consumer = consumer.reset_index(drop=True)
ganti_re = re.compile('[/(){}\[\]\|@,;]')
simbolburuk = re.compile('[^0-9a-z #+_]')
STOPWORDS = set(stopwords.words('english'))

def bersihkan_teks(teks):
    """
        text: a string
        
        return: modified initial string
    """
    teks = str(teks).lower() 
    teks = ganti_re.sub(' ', teks)
    teks = simbolburuk.sub('', teks) 
    teks = teks.replace('x', '')

    teks = ' '.join(word for word in teks.split() if word not in STOPWORDS)
    return teks
consumer['Issue'] = consumer['Issue'].apply(bersihkan_teks)

consumer['Issue'] = consumer['Issue'].str.replace('\d+', '')

maks_kata = 50000

maks_length = 250

embedding_dim= 100

tokenizer = Tokenizer(num_words=maks_kata, filters='!"#$%&()*+,-./:;<=>?@[\]^_`{|}~', lower=True)
tokenizer.fit_on_texts(consumer['Issue'].values)
word_index = tokenizer.word_index
print('Menemukan %s token unik.' % len(word_index))

X = tokenizer.texts_to_sequences(consumer['Issue'].values)
X = pad_sequences(X, maxlen=maks_length)
print('Ukuran tensor x : ', X.shape)

Y = pd.get_dummies(consumer['Product']).values
print('Ukuran tensor y:', Y.shape)

X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20, random_state = 42)
print(X_train.shape,Y_train.shape)
print(X_test.shape,Y_test.shape)

model = Sequential()
model.add(Embedding(maks_kata, embedding_dim, input_length=X.shape[1]))
model.add(SpatialDropout1D(0.2))
model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(11, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model.summary())

epochs = 5
batch_size = 256

history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])

akurasi = model.evaluate(X_test,Y_test)
print('Test set\n  Loss: {:0.3f}\n  Accuracy: {:0.3f}'.format(akurasi[0],akurasi[1]))

plt.title('Loss')
plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='test')
plt.legend()
plt.show();

plt.title('Accuracy')
plt.plot(history.history['accuracy'], label='train')
plt.plot(history.history['val_accuracy'], label='test')
plt.legend()
plt.show();

"""### **Lampiran DATA DIRI di dicoding**"""

import matplotlib.image as mpimg
img = mpimg.imread('/content/Data diri Alifia Feiling.jpg')
imgplot = plt.imshow(img)
plt.show()

